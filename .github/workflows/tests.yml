name: CI

on:
  push:
    branches: [main]
    paths-ignore:
      - '**.md'
  pull_request:
    types: [opened, reopened, synchronize, labeled]
    paths-ignore:
      - '**.md'
  # Manually trigger a workflow for a branch
  workflow_dispatch:
  # Merge queue trigger
  merge_group:

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: registry.gitlab.aleph-alpha.de
  REPO_OWNER: research/public-registry
  IMAGE_NAME: eval_framework
  HF_DATASET_CACHE_DIR: /tmp/huggingface_datasets  # <- single source of truth
  UV_LINK_MODE: symlink
  UV_LOCKED: 1

jobs:
  lint:
    runs-on: ubuntu-latest # default runner runs out of disk space due to hf cache
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup uv
        uses: astral-sh/setup-uv@v6
        with:
          version: "~=0.8.16"

      - name: Run Pre-Commit
        run: uvx pre-commit run --all-files

      - name: Dependency check
        run: ./utils/dependency_check.sh

      - name: Run MyPy
        run: uv run --all-extras mypy

  hf-datasets-cache:
    runs-on: cpu-runner-8c-32gb-01  # default runner runs out of disk space, unfortunately
    steps:
      - uses: actions/checkout@v4
        if: github.ref == 'refs/heads/main'

      - name: Setup uv
        uses: astral-sh/setup-uv@v6
        if: github.ref == 'refs/heads/main'
        with:
          version: "~=0.8.16"

      - name: Huggingface datasets cache
        uses: actions/cache@v4
        if: github.ref == 'refs/heads/main'
        with:
          path: ${{ env.HF_DATASET_CACHE_DIR }}        # <- use shared env
          key: hf-datasets-${{ github.run_id }}
          restore-keys: |
            hf-datasets-

      - name: Download datasets
        if: github.ref == 'refs/heads/main'
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_API_KEY }}
        run: uv run --extra=comet python -c "from eval_framework.tasks.task_names import make_sure_all_hf_datasets_are_in_cache; make_sure_all_hf_datasets_are_in_cache()"

  tag:
    name: Set Docker Tag and Image Name for Docker Build and Push (GPU Runs)
    runs-on: ubuntu-latest
    outputs:
      tag: ${{ steps.set-tag.outputs.tag }}
      image: ${{ steps.set-tag.outputs.image }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    - name: Set Tag
      id: set-tag
      run: |
        if [ "${{ github.ref }}" == "refs/heads/main" ]; then
          TAG='latest'
        else
          # head_ref is the correct branch name for PRs
          BRANCH_NAME=${{ github.head_ref || github.ref_name }}
          # Convert slashes with hyphens and ensure valid Docker tag format
          TAG=$(echo "${BRANCH_NAME}" | sed 's/[^a-zA-Z0-9._-]/-/g' | cut -c1-20)
        fi
        echo "tag=$TAG" >> $GITHUB_OUTPUT
        echo "image=${{ env.REGISTRY }}/${{ env.REPO_OWNER }}/${{ env.IMAGE_NAME }}:$TAG" >> $GITHUB_OUTPUT

    - name: Output Docker Tag
      run: |
        echo "Docker Tag: ${{ steps.set-tag.outputs.tag }}"
        echo "Docker image: ${{ steps.set-tag.outputs.image }}"

  build:
    name: Build and Push Docker Image (GPU Runs)
    needs: [lint, tag]
    runs-on: cpu-runner-8c-32gb-01
    container: docker:dind
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Registry Authentication
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: token
          password: ${{ secrets.GL_PUBLIC_REGISTRY_READ_WRITE_TOKEN }}

      - name: Setup Docker BuildX
        uses: docker/setup-buildx-action@v1

      - name: Build and Push Image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile
          push: true
          tags: ${{ needs.tag.outputs.image }}

  test-pip-install:
    name: Test pip install in clean venv
    runs-on: ubuntu-latest
    needs: [lint]
    strategy:
      fail-fast: false
      matrix:
        extras: ['', '[determined]', '[api]', '[openai]', '[transformers]', '[accelerate]', '[comet]', '[optional]', '[mistral]', '[all]']
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Build package
        run: |
          python -m pip install --upgrade pip build
          python -m build --outdir dist/

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Create self-contained venv and install via pip
        run: |
          python -m venv isolated_venv
          source isolated_venv/bin/activate
          uv pip install $(ls dist/*.whl)${{ matrix.extras }}
          pip show eval-framework

      - name: Verify core functionality
        run: |
          source isolated_venv/bin/activate

          # Test imports
          python -c "import eval_framework; print(f'✓ Version: {eval_framework.__version__}')"
          python -c "from eval_framework.llm.base import BaseLLM; print('✓ BaseLLM')"
          python -c "from eval_framework.tasks.base import BaseTask; print('✓ BaseTask')"
          python -c "from template_formatting.formatter import HFFormatter; print('✓ HFFormatter')"

          # Test CLI
          eval_framework --help > /dev/null && echo "✓ CLI works"

      - name: Verify extras functionality
        if: matrix.extras != ''
        run: |
          source isolated_venv/bin/activate

          case "${{ matrix.extras }}" in
            "[determined]")
              python -c "import determined; print('Determined version:', determined.__version__)"
              python -c "from eval_framework.context.determined import DeterminedContext; print('✓ determined')"
              ;;
            "[api]")
              python -c "from eval_framework.llm.aleph_alpha import AlephAlphaAPIModel; print('✓ api')"
              ;;
            "[openai]")
              python -c "from eval_framework.llm.openai import OpenAI; print('✓ openai')"
              ;;
            "[transformers]")
              python -c "from eval_framework.llm.huggingface import HFLLM; print('✓ transformers')"
              ;;
            "[accelerate]")
              python -c "import accelerate; print('Accelerate version:', accelerate.__version__)"
              echo "Running 'accelerate env'..."
              accelerate env
              ;;
            "[comet]")
              # Test task registry (comet required for FloresPlus tasks)
              python -c "from eval_framework.tasks.task_names import registered_tasks_iter; print(f'✓ {len(list(registered_tasks_iter()))} tasks loaded, comet extra works')"
              ;;
            "[optional]")
              python -c "from jinja2 import Template; print('Jinja2 OK:', Template('Hello {{ name }}!').render(name='World'))"
              python -c "from eval_framework.llm.huggingface import HFLLM; print('✓ transformers')"
              ;;
            "[all]")
              python -c "from eval_framework.llm.huggingface import HFLLM"
              python -c "from eval_framework.llm.openai import OpenAI"

              # Skip VLLM import since no GPU in CI
              echo "[INFO] Skipping VLLM import (no CUDA available)"

              # Test task registry (comet required for FloresPlus tasks)
              python -c "from eval_framework.tasks.task_names import registered_tasks_iter; print(f'✓ {len(list(registered_tasks_iter()))} tasks loaded')"

              echo "✓ All extras verified"
              ;;
          esac

  test-pip-install-gpu:
    name: Test pip install eval_framework in clean venv (GPU Run)
    runs-on: EvalFrameworkGPURunner
    needs: [tag, build, test-pip-install]
    container:
      image: "${{ needs.tag.outputs.image }}"
      credentials:
        username: token
        password: ${{ secrets.GL_PUBLIC_REGISTRY_READ_WRITE_TOKEN }}
      options: --gpus all
    strategy:
      fail-fast: false
      matrix:
        extras: ['[mistral]', '[vllm]']
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Ensure pip is available
        run: python -m ensurepip --upgrade

      - name: Build package
        run: |
          python -m pip install --upgrade pip build
          python -m build --outdir dist/

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Create self-contained venv with vllm and install via pip
        shell: bash
        run: |
          python -m venv isolated_venv
          source isolated_venv/bin/activate
          uv pip install $(ls dist/*.whl)${{ matrix.extras }}
          pip show eval-framework

      - name: Verify core functionality
        run: |
          source isolated_venv/bin/activate

          # Test imports
          python -c "import eval_framework; print(f'✓ Version: {eval_framework.__version__}')"
          python -c "from eval_framework.llm.base import BaseLLM; print('✓ BaseLLM')"
          python -c "from eval_framework.tasks.base import BaseTask; print('✓ BaseTask')"
          python -c "from template_formatting.formatter import HFFormatter; print('✓ HFFormatter')"

          # Test CLI
          eval_framework --help > /dev/null && echo "✓ CLI works"

      - name: Verify extras functionality
        if: matrix.extras != ''
        run: |
          source isolated_venv/bin/activate

          case "${{ matrix.extras }}" in
            "[vllm]")
              python -c "from eval_framework.llm.vllm import VLLMModel; print('✓ vllm')"
              ;;
            "[mistral]")
              python -c "from eval_framework.llm.mistral import MistralVLLM; print('✓ mistral')"
              echo "✓ All extras verified"
              ;;
          esac

  test-cpu:
    runs-on: cpu-runner-8c-32gb-01
    container: derskythe/github-runner-base:ubuntu-noble
    needs: [hf-datasets-cache, test-pip-install]
    steps:
      - uses: actions/checkout@v4

      - name: Setup uv
        uses: astral-sh/setup-uv@v6
        with:
          version: "~=0.8.16"

      - name: Huggingface datasets cache
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.HF_DATASET_CACHE_DIR }}        # <- shared path
          key: hf-datasets-

      - name: Run tests
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_API_KEY }}
        run: uv run --all-extras pytest --durations=30 -v -m "not gpu and not cpu_slow and not external_api"

  test-cpu-slow:
    runs-on: cpu-runner-8c-32gb-01
    container: derskythe/github-runner-base:ubuntu-noble
    needs: [hf-datasets-cache, test-pip-install]
    steps:
      - uses: actions/checkout@v4

      - name: Setup uv
        uses: astral-sh/setup-uv@v6
        with:
          version: "~=0.8.16"

      - name: Huggingface datasets cache
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.HF_DATASET_CACHE_DIR }}        # <- shared path
          key: hf-datasets-

      - name: Run tests
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_API_KEY }}
        run: |
          uv run --all-extras python -c "import nltk; nltk.download('punkt_tab')"  # otherwise there's a race condition in ntltk
          uv run --all-extras pytest -n auto --max-worker-restart=0 --durations=30 -v -m "not gpu and cpu_slow and not external_api"

  test-docker-gpu:
    name: Run full test suite in Docker Container with GPU
    runs-on: EvalFrameworkGPURunner
    needs: [tag, test-cpu, test-cpu-slow, test-pip-install-vllm]
    container:
      image: "${{ needs.tag.outputs.image }}"
      credentials:
        username: token
        password: ${{ secrets.GL_PUBLIC_REGISTRY_READ_WRITE_TOKEN }}
      options: --gpus all
    defaults:
      run:
        working-directory: /eval_framework
    steps:
      - name: Huggingface datasets cache
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.HF_DATASET_CACHE_DIR }}        # <- shared path
          key: hf-datasets-

      - name: Test GPU
        timeout-minutes: 20
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_API_KEY }}
        run: pytest --durations=30 -v -m "gpu and not cpu_slow and not external_api and not vllm"

      - name: Test VLLM
        timeout-minutes: 20
        env:
          HF_TOKEN: ${{ secrets.HUGGINGFACE_API_KEY }}
          VLLM_LOGGING_LEVEL: DEBUG
          VLLM_WORKER_MULTIPROC_METHOD: spawn
          VLLM_USE_MODELSCOPE: False
          VLLM_NCCL_SO_PATH: ""
          VLLM_USE_TRITON_FLASH_ATTN: 0
          VLLM_DISABLE_CUSTOM_ALL_REDUCE: 1
        run: pytest --log-cli-level=INFO -v -m "vllm"
