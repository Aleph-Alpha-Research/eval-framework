{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Basic Installation\n",
    "```bash\n",
    "pip install eval-framework\n",
    "```\n",
    "\n",
    "### With HuggingFace Support\n",
    "```bash\n",
    "pip install eval-framework[transformers,accelerate]\n",
    "```\n",
    "\n",
    "### With All Features\n",
    "```bash\n",
    "pip install eval-framework[all]\n",
    "```\n",
    "\n",
    "**Includes**: OpenAI, vLLM, Mistral, COMET metrics, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Running an Evaluation\n",
    "\n",
    "Simple evaluation of GSM8K on a HuggingFace SmolLM-360M-Instruct model, with 5 few-shots and 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "eval_framework \\\n",
    "    --llm-name eval_framework.llm.huggingface.HFLLM_from_name \\\n",
    "    --llm-args model_name=\"HuggingFaceTB/SmolLM-360M-Instruct\" \\\n",
    "    --task-name \"MMLU\" \\\n",
    "    --output-dir ./eval \\\n",
    "    --num-fewshot 5 \\\n",
    "    --num-samples 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Structure\n",
    "\n",
    "After running an evaluation, you get:\n",
    "\n",
    "```\n",
    "./eval/\n",
    "├── aggregated_results.json    # Final metrics\n",
    "├── results.jsonl              # Per-sample metrics\n",
    "├── output.jsonl               # Model completions\n",
    "├── metadata.json              # Run configuration\n",
    "└── evaluation.log             # Detailed logs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval-Framework Features\n",
    "\n",
    "### 1. **Flexible Model Support**\n",
    "```bash\n",
    "# HuggingFace Transformers\n",
    "--llm-name eval_framework.llm.huggingface.HFLLM_from_name \\\n",
    "--llm-args model_name=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Predefined models from registry\n",
    "--llm-name Smollm135MInstruct\n",
    "\n",
    "# vLLM for high-performance inference (10-20x faster)\n",
    "--llm-name eval_framework.llm.vllm.VLLM \\\n",
    "--llm-args model_name=\"meta-llama/Llama-3.2-8B-Instruct\"\n",
    "\n",
    "# OpenAI API models\n",
    "--llm-name eval_framework.llm.openai.OpenAIModel \\\n",
    "--llm-args model_name=\"gpt-4o\"\n",
    "\n",
    "# Mistral AI models\n",
    "--llm-name eval_framework.llm.mistral.MistralModel \\\n",
    "--llm-args model_name=\"mistral-large-latest\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 2. **Built-in Benchmarks**\n",
    "```bash\n",
    "# Math & Reasoning: GSM8K, MATH, MMLU (57 subjects), ARC, HellaSwag\n",
    "--task-name \"GSM8K\"\n",
    "\n",
    "# Code Generation: HumanEval, MBPP, BigCodeBench\n",
    "--task-name \"HumanEval\"\n",
    "\n",
    "# Long Context: InfiniteBench, ZeroScrolls, QUALITY\n",
    "--task-name \"InfiniteBench_CodeDebug\"\n",
    "\n",
    "# Translation: WMT14/16/20, Flores200, FloresPlus\n",
    "--task-name \"WMT20\"\n",
    "\n",
    "# Multilingual: BELEBELE (122 languages), MMLU-DE, ARC-FI\n",
    "--task-name \"BELEBELE\"\n",
    "\n",
    "# Safety: TruthfulQA, Instruction Following (IFEval)\n",
    "--task-name \"TruthfulQA\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 3. **Few-Shot Learning**\n",
    "```bash\n",
    "--num-fewshot 5  # Add 5 examples to each prompt\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 4. **LLM-as-Judge Metrics**\n",
    "```bash\n",
    "# Evaluate with GPT-4 as judge\n",
    "--llm-name eval_framework.llm.huggingface.HFLLM_from_name \\\n",
    "--llm-args model_name=\"meta-llama/Llama-3.2-1B-Instruct\" \\\n",
    "--judge-model-name eval_framework.llm.openai.OpenAIModel \\\n",
    "--judge-model-args model_name=\"gpt-4o\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 5. **Batch Processing**\n",
    "```bash\n",
    "# Parallel batch processing\n",
    "--batch-size 8\n",
    "\n",
    "# Multi-GPU inference (vLLM)\n",
    "--llm-args tensor_parallel_size=4\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 6. **Experiment Tracking & Reproducibility**\n",
    "```bash\n",
    "# Weights & Biases integration\n",
    "--wandb-project \"llm-evaluations\" \\\n",
    "--wandb-entity \"my-team\" \\\n",
    "--description \"Baseline Llama-3.2-1B evaluation\"\n",
    "\n",
    "# HuggingFace Hub uploads\n",
    "--hf-upload-repo \"my-org/eval-results\" \\\n",
    "--hf-upload-dir \"experiment-1\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 7. **Evaluation Options**\n",
    "```bash\n",
    "# Control generation parameters\n",
    "--max-tokens 256\n",
    "--llm-args sampling_params.temperature=0.7 \\\n",
    "--llm-args sampling_params.top_p=0.9\n",
    "\n",
    "# Filter specific subjects (e.g., MMLU math only)\n",
    "--task-subjects \"abstract_algebra\" \"college_mathematics\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 8. **Robustness Testing**\n",
    "```bash\n",
    "# Add character-level perturbations\n",
    "--perturbation-type \"replace\" \\\n",
    "--perturbation-probability 0.05 \\\n",
    "--perturbation-seed 42\n",
    "\n",
    "# Available: editor, permute, replace, delete, uppercase\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
